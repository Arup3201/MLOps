{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sequential Network\n",
    "Definition:\n",
    "A Sequential model is a linear stack of layers. It is the simplest type of neural network model, suitable for feedforward networks.\n",
    "\n",
    "2. Dense Layer\n",
    "Definition:\n",
    "A Dense layer, also known as a fully connected layer, where each neuron receives input from all neurons of the previous layer. This layer performs a linear transformation of the input vector.\n",
    "\n",
    "Properties:\n",
    "\n",
    "Units: Number of neurons in the layer.\n",
    "Input Dimension: The shape of the input data. Required only for the first layer.\n",
    "3. Weights and Initialization Techniques\n",
    "Weights:\n",
    "Weights are the parameters within the neural network that get updated during training. They connect the neurons of one layer to the neurons of the next layer.\n",
    "\n",
    "Initialization Techniques:\n",
    "Proper initialization is crucial for training efficiency and convergence. Common initialization methods include:\n",
    "\n",
    "Zero Initialization:\n",
    "\n",
    "Description: All weights are initialized to zero.\n",
    "Disadvantage: Leads to symmetry and poor learning.\n",
    "Random Initialization:\n",
    "\n",
    "Description: Weights are initialized to small random values.\n",
    "Disadvantage: May cause slow convergence.\n",
    "He Initialization:\n",
    "\n",
    "Description: Weights are initialized based on a normal distribution with a mean of 0 and a variance of \\frac{2}{\\text{fan_in}}.\n",
    "Usage: Suitable for ReLU and its variants.\n",
    "Formula: W \\sim \\mathcal{N}(0, \\sqrt{\\frac{2}{\\text{fan_in}}})\n",
    "Xavier/Glorot Initialization:\n",
    "\n",
    "Description: Weights are initialized based on a normal distribution with a mean of 0 and a variance of \\frac{2}{\\text{fan_in} + \\text{fan_out}}.\n",
    "Usage: Suitable for sigmoid and tanh activations.\n",
    "Formula: W \\sim \\mathcal{N}(0, \\sqrt{\\frac{2}{\\text{fan_in} + \\text{fan_out}}})\n",
    "4. Activation Functions\n",
    "Definition:\n",
    "Activation functions introduce non-linearity into the network, allowing it to learn and model complex data.\n",
    "\n",
    "Categories:\n",
    "\n",
    "Sigmoid Activation Function:\n",
    "\n",
    "Formula: \n",
    "σ\n",
    "(\n",
    "x\n",
    ")\n",
    "=\n",
    "1\n",
    "1\n",
    "+\n",
    "e\n",
    "−\n",
    "x\n",
    "σ(x)= \n",
    "1+e \n",
    "−x\n",
    " \n",
    "1\n",
    "​\t\n",
    " \n",
    "Range: (0, 1)\n",
    "Usage: Often used in the output layer of binary classification problems.\n",
    "ReLU (Rectified Linear Unit):\n",
    "\n",
    "Formula: \n",
    "f\n",
    "(\n",
    "x\n",
    ")\n",
    "=\n",
    "max\n",
    "⁡\n",
    "(\n",
    "0\n",
    ",\n",
    "x\n",
    ")\n",
    "f(x)=max(0,x)\n",
    "Range: [0, ∞)\n",
    "Usage: Widely used in hidden layers for its simplicity and efficiency.\n",
    "Tanh (Hyperbolic Tangent):\n",
    "\n",
    "Formula: \n",
    "tanh\n",
    "⁡\n",
    "(\n",
    "x\n",
    ")\n",
    "=\n",
    "e\n",
    "x\n",
    "−\n",
    "e\n",
    "−\n",
    "x\n",
    "e\n",
    "x\n",
    "+\n",
    "e\n",
    "−\n",
    "x\n",
    "tanh(x)= \n",
    "e \n",
    "x\n",
    " +e \n",
    "−x\n",
    " \n",
    "e \n",
    "x\n",
    " −e \n",
    "−x\n",
    " \n",
    "​\t\n",
    " \n",
    "Range: (-1, 1)\n",
    "Usage: Often used in hidden layers. Good for handling negative input values.\n",
    "Softmax:\n",
    "\n",
    "Formula: \n",
    "σ\n",
    "(\n",
    "z\n",
    ")\n",
    "i\n",
    "=\n",
    "e\n",
    "z\n",
    "i\n",
    "∑\n",
    "j\n",
    "=\n",
    "1\n",
    "K\n",
    "e\n",
    "z\n",
    "j\n",
    "σ(z) \n",
    "i\n",
    "​\t\n",
    " = \n",
    "∑ \n",
    "j=1\n",
    "K\n",
    "​\t\n",
    " e \n",
    "z \n",
    "j\n",
    "​\t\n",
    " \n",
    " \n",
    "e \n",
    "z \n",
    "i\n",
    "​\t\n",
    " \n",
    " \n",
    "​\t\n",
    " \n",
    "Range: [0, 1] (sum to 1 across the output neurons)\n",
    "Usage: Used in the output layer of multiclass classification problems.\n",
    "5. Forward Propagation\n",
    "Definition:\n",
    "Forward propagation is the process of passing input data through the network to obtain an output prediction.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Input Layer: Receive input data.\n",
    "Weighted Sum: Calculate the weighted sum of inputs for each neuron in the next layer.\n",
    "z\n",
    "=\n",
    "W\n",
    "⋅\n",
    "x\n",
    "+\n",
    "b\n",
    "z=W⋅x+b\n",
    "Activation: Apply activation function.\n",
    "a\n",
    "=\n",
    "σ\n",
    "(\n",
    "z\n",
    ")\n",
    "a=σ(z)\n",
    "Output Layer: Produce the final output after passing through all layers.\n",
    "Equation:\n",
    "For a single layer:\n",
    "a\n",
    "[\n",
    "l\n",
    "]\n",
    "=\n",
    "σ\n",
    "(\n",
    "W\n",
    "[\n",
    "l\n",
    "]\n",
    "⋅\n",
    "a\n",
    "[\n",
    "l\n",
    "−\n",
    "1\n",
    "]\n",
    "+\n",
    "b\n",
    "[\n",
    "l\n",
    "]\n",
    ")\n",
    "a \n",
    "[l]\n",
    " =σ(W \n",
    "[l]\n",
    " ⋅a \n",
    "[l−1]\n",
    " +b \n",
    "[l]\n",
    " )\n",
    "Where:\n",
    "\n",
    "a\n",
    "[\n",
    "l\n",
    "]\n",
    "a \n",
    "[l]\n",
    " : Activation of layer \n",
    "l\n",
    "l\n",
    "W\n",
    "[\n",
    "l\n",
    "]\n",
    "W \n",
    "[l]\n",
    " : Weights of layer \n",
    "l\n",
    "l\n",
    "b\n",
    "[\n",
    "l\n",
    "]\n",
    "b \n",
    "[l]\n",
    " : Biases of layer \n",
    "l\n",
    "l\n",
    "σ\n",
    "σ: Activation function\n",
    "6. Backpropagation\n",
    "Definition:\n",
    "Backpropagation is the process of updating the weights and biases to minimize the loss function by propagating the error backwards through the network.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Compute Loss: Calculate the difference between predicted and actual values using a loss function.\n",
    "Calculate Gradients: Compute gradients of the loss function with respect to weights and biases using the chain rule.\n",
    "Update Weights: Adjust weights and biases using the gradients and a learning rate.\n",
    "Equations:\n",
    "\n",
    "Loss Function Gradient:\n",
    "∂\n",
    "L\n",
    "∂\n",
    "W\n",
    "[\n",
    "l\n",
    "]\n",
    "∂W \n",
    "[l]\n",
    " \n",
    "∂L\n",
    "​\t\n",
    " \n",
    "∂\n",
    "L\n",
    "∂\n",
    "b\n",
    "[\n",
    "l\n",
    "]\n",
    "∂b \n",
    "[l]\n",
    " \n",
    "∂L\n",
    "​\t\n",
    " \n",
    "Weight Update:\n",
    "W\n",
    "[\n",
    "l\n",
    "]\n",
    "=\n",
    "W\n",
    "[\n",
    "l\n",
    "]\n",
    "−\n",
    "α\n",
    "⋅\n",
    "∂\n",
    "L\n",
    "∂\n",
    "W\n",
    "[\n",
    "l\n",
    "]\n",
    "W \n",
    "[l]\n",
    " =W \n",
    "[l]\n",
    " −α⋅ \n",
    "∂W \n",
    "[l]\n",
    " \n",
    "∂L\n",
    "​\t\n",
    " \n",
    "b\n",
    "[\n",
    "l\n",
    "]\n",
    "=\n",
    "b\n",
    "[\n",
    "l\n",
    "]\n",
    "−\n",
    "α\n",
    "⋅\n",
    "∂\n",
    "L\n",
    "∂\n",
    "b\n",
    "[\n",
    "l\n",
    "]\n",
    "b \n",
    "[l]\n",
    " =b \n",
    "[l]\n",
    " −α⋅ \n",
    "∂b \n",
    "[l]\n",
    " \n",
    "∂L\n",
    "​\t\n",
    " \n",
    "Where \n",
    "α\n",
    "α is the learning rate.\n",
    "\n",
    "7. Optimizers\n",
    "Definition:\n",
    "Optimizers are algorithms used to adjust the weights of the network to minimize the loss function.\n",
    "\n",
    "Categories:\n",
    "\n",
    "Stochastic Gradient Descent (SGD):\n",
    "\n",
    "Description: Updates the weights based on the gradient of the loss function.\n",
    "Learning Rate: Hyperparameter that controls the step size.\n",
    "Adam (Adaptive Moment Estimation):\n",
    "\n",
    "Description: Combines the advantages of two other extensions of SGD, specifically AdaGrad and RMSProp.\n",
    "Parameters: Learning rate, beta1, beta2.\n",
    "RMSprop:\n",
    "\n",
    "Description: Adapts the learning rate for each parameter.\n",
    "Usage: Suitable for non-stationary objectives.\n",
    "Adagrad:\n",
    "\n",
    "Description: Adapts the learning rate to the parameters, performing larger updates for infrequent parameters and smaller updates for frequent ones.\n",
    "8. Loss Functions\n",
    "Definition:\n",
    "Loss functions measure how well the model's predictions match the true data labels. The choice of loss function depends on the type of problem being solved.\n",
    "\n",
    "Types:\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "\n",
    "Usage: Regression problems.\n",
    "Formula: \n",
    "MSE\n",
    "=\n",
    "1\n",
    "n\n",
    "∑\n",
    "i\n",
    "=\n",
    "1\n",
    "n\n",
    "(\n",
    "y\n",
    "i\n",
    "−\n",
    "y\n",
    "i\n",
    "^\n",
    ")\n",
    "2\n",
    "MSE= \n",
    "n\n",
    "1\n",
    "​\t\n",
    " ∑ \n",
    "i=1\n",
    "n\n",
    "​\t\n",
    " (y \n",
    "i\n",
    "​\t\n",
    " − \n",
    "y \n",
    "i\n",
    "​\t\n",
    " \n",
    "^\n",
    "​\t\n",
    " ) \n",
    "2\n",
    " \n",
    "Binary Crossentropy:\n",
    "\n",
    "Usage: Binary classification problems.\n",
    "Formula: \n",
    "Binary Crossentropy\n",
    "=\n",
    "−\n",
    "1\n",
    "n\n",
    "∑\n",
    "i\n",
    "=\n",
    "1\n",
    "n\n",
    "[\n",
    "y\n",
    "i\n",
    "log\n",
    "⁡\n",
    "(\n",
    "y\n",
    "i\n",
    "^\n",
    ")\n",
    "+\n",
    "(\n",
    "1\n",
    "−\n",
    "y\n",
    "i\n",
    ")\n",
    "log\n",
    "⁡\n",
    "(\n",
    "1\n",
    "−\n",
    "y\n",
    "i\n",
    "^\n",
    ")\n",
    "]\n",
    "Binary Crossentropy=− \n",
    "n\n",
    "1\n",
    "​\t\n",
    " ∑ \n",
    "i=1\n",
    "n\n",
    "​\t\n",
    " [y \n",
    "i\n",
    "​\t\n",
    " log( \n",
    "y \n",
    "i\n",
    "​\t\n",
    " \n",
    "^\n",
    "​\t\n",
    " )+(1−y \n",
    "i\n",
    "​\t\n",
    " )log(1− \n",
    "y \n",
    "i\n",
    "​\t\n",
    " \n",
    "^\n",
    "​\t\n",
    " )]\n",
    "Categorical Crossentropy:\n",
    "\n",
    "Usage: Multiclass classification problems.\n",
    "Formula: \n",
    "Categorical Crossentropy\n",
    "=\n",
    "−\n",
    "∑\n",
    "i\n",
    "=\n",
    "1\n",
    "n\n",
    "y\n",
    "i\n",
    "log\n",
    "⁡\n",
    "(\n",
    "y\n",
    "i\n",
    "^\n",
    ")\n",
    "Categorical Crossentropy=−∑ \n",
    "i=1\n",
    "n\n",
    "​\t\n",
    " y \n",
    "i\n",
    "​\t\n",
    " log( \n",
    "y \n",
    "i\n",
    "​\t\n",
    " \n",
    "^\n",
    "​\t\n",
    " )\n",
    "9. Metrics\n",
    "Definition:\n",
    "Metrics are used to evaluate the performance of the model. Unlike loss functions, metrics are not used for training the model but for monitoring and evaluating its performance.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "Usage: Suitable for classification problems.\n",
    "Description: Measures the percentage of correct predictions.\n",
    "Precision:\n",
    "\n",
    "Usage: Useful in binary classification to measure the accuracy of positive predictions.\n",
    "Formula: \n",
    "Precision\n",
    "=\n",
    "True Positives\n",
    "True Positives + False Positives\n",
    "Precision= \n",
    "True Positives + False Positives\n",
    "True Positives\n",
    "​\t\n",
    " \n",
    "Recall:\n",
    "\n",
    "Usage: Measures the ability of the model to find all relevant cases within a dataset.\n",
    "Formula: \n",
    "Recall\n",
    "=\n",
    "True Positives\n",
    "True Positives + False Negatives\n",
    "Recall= \n",
    "True Positives + False Negatives\n",
    "True Positives\n",
    "​\t\n",
    " \n",
    "F1 Score:\n",
    "\n",
    "Usage: Harmonic mean of Precision and Recall.\n",
    "Formula: \n",
    "F1 Score\n",
    "=\n",
    "2\n",
    "⋅\n",
    "Precision\n",
    "⋅\n",
    "Recall\n",
    "Precision + Recall\n",
    "F1 Score=2⋅ \n",
    "Precision + Recall\n",
    "Precision⋅Recall\n",
    "​\t\n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
